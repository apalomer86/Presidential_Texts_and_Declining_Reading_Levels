{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978b34c3-4eaa-40fe-b031-7e32465053aa",
   "metadata": {},
   "source": [
    "# **Download Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027dbc73-2b10-4c60-b586-9cf6a1130cb8",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffca5e2-15f1-4fb0-8300-7e98163844c0",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### **Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2c5da-9d05-455a-9cde-1770973a134f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe0d6ab-b367-4371-b710-aad7951e9acc",
   "metadata": {},
   "source": [
    "Data for our analysis over presidential text and decreasing readability levels was scraped from a variety of online resources. This notebook provides a brief explanation of each dataset used along with the code used to obtain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1c9a5-6e72-4e12-9cf0-22b9b85ea440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from random import uniform\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ace65af-275e-425a-a314-61f20db9c421",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5f281-effe-450c-b146-6fb576c1740e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78308640-2687-46b6-8dfc-41fa0fde6b84",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### **NAEP Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3f800-e4be-47cc-91b7-949573fe9b52",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b9fdf4-b945-475a-b0cb-02213e8339cc",
   "metadata": {},
   "source": [
    "[NAEP](https://nces.ed.gov/nationsreportcard/https://nces.ed.gov/nationsreportcard/),\n",
    "or the National Assessment of Educational Progress, is a national program used to measure student achievement across various core subject areas. Data from these assessments is used to generate\n",
    "[the Nation's Report Card](https://www.nationsreportcard.gov/https://www.nationsreportcard.gov/),\n",
    "which can be used to assess how well students are meeting targeted learning goals at national, state, and urban-district levels. For the purposes of this project, we examined results from the NAEP assessments in reading comprehension—NAEP Main and NAEP LTT (Long-Term Trends)—as a proxy for average reading levels of young American adults upon exiting the K-12 education system. Scores for both assessments are given on a\n",
    "$0-500$\n",
    "scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d07c99-a4bd-4712-b593-5994070b9593",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338661fc-7430-461d-bad9-6bbdeb9a2cf9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45efb3f2-642f-4a4d-b419-6263d571a588",
   "metadata": {},
   "source": [
    "#### **NAEP Main Data, National Reading, Grade 12**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4417e-2168-46f7-ae3c-97d72009adfd",
   "metadata": {},
   "source": [
    "Beginning in 1992, NAEP Main has been administered to students at grades 4, 8, and 12. For our project, we gathered data on average scale NAEP reading scores for 12th graders from 1992 to 2024. Data was aggregated on an national level and did not account for demographic or geographic differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53da2fa-2091-4133-9d7c-264fd0529311",
   "metadata": {},
   "source": [
    "Visit the [Nations's Report Card](https://nces.ed.gov/nationsreportcard/reading/achieve.aspx#2009ald)\n",
    "website for more detailed explanations on the meanings of the scores. Note that although cutoff scores for performance levels have remained the same since 1992, different descriptors are given for scores from 1992-2007 and for 2007-to present.\n",
    "\n",
    "| NAEP Main Average Scale Score | Performance Level                            |\n",
    "|:-----------------------------:|----------------------------------------------|\n",
    "| $265$                         | NAEP Basic                                   |\n",
    "| $302$                         | NAEP Proficient                              |\n",
    "| $346$                         | NAEP Advanced                                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a2638c-2f08-4d40-a4a1-ba1448abfd39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    \"data/NAEP-Main, Reading, Grade 12.Xls\",\n",
    "    skiprows = list(range(0, 8)) + list(range(20, 22))\n",
    ")\n",
    "\n",
    "# Add column for \"Accommodations Allowed\"\n",
    "n_rows = len(df)\n",
    "accommodations = [\"No\"] * 3 + [\"Yes\"] * (n_rows - 3)\n",
    "df.insert(loc = 1, column = \"Accommodations Allowed\", value = accommodations)\n",
    "df = df.iloc[:-2]\n",
    "\n",
    "# Add \"Grade Level\"\n",
    "n_rows = len(df)\n",
    "grade_level = 12\n",
    "df.insert(loc = 3, column = \"Grade Level\", value = grade_level)\n",
    "\n",
    "# Remove any non-digit characters\n",
    "df[\"Year\"] = df[\"Year\"].astype(str).str.replace(r\"\\D\", \"\", regex = True)\n",
    "\n",
    "# Convert \"Year\" to integer\n",
    "df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "\n",
    "df.to_csv(\"data/NAEP-Main, Reading, Grade 12.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b47ac-e7a2-49dd-8905-1d2adc99d78e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25976ead-58b7-4433-bbd1-e69af11636f4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1e6c3-677a-49eb-b5d2-527161006f97",
   "metadata": {},
   "source": [
    "#### **NAEP LTT (Long-Term Trends), National Reading, Age 17**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce6af6-fff0-4c10-9e68-5376176c3006",
   "metadata": {},
   "source": [
    "From 1971 and onwards, NAEP LTT has been administered roughly every four years to students at ages 9, 13, and 17. For our project, we gathered data on average scale NAEP reading scores for 17-year-olds from 1971 to 2012. Data was aggregated on an national level and did not account for demographic or geographic differences.\n",
    "\n",
    "Visit the [Nations's Report Card](https://nces.ed.gov/nationsreportcard/reading/achieve.aspx#2009ald)\n",
    "website for more detailed explanations on the meanings of the scores. Because NAEP LTT assessments have remained relatively unchanged since 1992, descriptions for performance levels have not changed in the same way as NAEP Main.\n",
    "\n",
    "| **NAEP LTT Average Scale Score**  | **Performance Level**                                    |\n",
    "|:---------------------------------:|----------------------------------------------------------|\n",
    "| $350$                             | Learn from Specialized Reading Materials                 |\n",
    "| $300$                             | Understand Complicated Information                       |\n",
    "| $250$                             | Interrelate Ideas and Make Generalizations               |\n",
    "| $200$                             | Demonstrate Partially Developed Skills and Understanding |\n",
    "| $150$                             | Carry Out Simple, Discrete Reading Tasks                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b35c5-430b-4a93-a701-3d2096274898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    \"data/NAEP-LTT, Reading, Age 17.Xls\",\n",
    "    skiprows = list(range(0, 8)) + list(range(24, 26))\n",
    ")\n",
    "\n",
    "# Add column for \"Original Assessment Format\"\n",
    "n_rows = len(df)\n",
    "original_assessment_format = [\"Yes\"] * 11 + [\"No\"] * (n_rows - 11)\n",
    "df.insert(loc = 1, column = \"Original Assessment Format\", value = original_assessment_format)\n",
    "df = df.iloc[:-2]\n",
    "\n",
    "# Add \"Age\"\n",
    "n_rows = len(df)\n",
    "age = 17\n",
    "df.insert(loc = 3, column = \"Age\", value = age)\n",
    "\n",
    "# Remove any non-digit characters\n",
    "df[\"Year\"] = df[\"Year\"].astype(str).str.replace(r\"\\D\", \"\", regex = True)\n",
    "\n",
    "# Convert \"Year\" to integer\n",
    "df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "\n",
    "df.to_csv(\"data/NAEP-LTT, Reading, Age 17.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe36673-194a-458a-a71f-02c5038013ec",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0767ef-9c4c-4add-9f70-ae4197cfbd05",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ecc8cf-840c-4dd3-aefa-a9d613784c40",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### **US Presidents by Political Party and Years in Office (Britannica)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0eb641-4dc1-4d89-adaa-d38430f4c552",
   "metadata": {},
   "source": [
    "---\n",
    "Data on US presidents was taken from this\n",
    "[Brittanica webpage](https://www.britannica.com/topic/Presidents-of-the-United-States-1846696https:). It contains president names, political party, and years in office."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7076ad-8056-483c-a116-64cd6ea1c6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://www.britannica.com/topic/Presidents-of-the-United-States-1846696\"\n",
    "resp = requests.get(url)\n",
    "resp.raise_for_status()\n",
    "\n",
    "tables = pd.read_html(resp.text)\n",
    "df = tables[0]\n",
    "df.to_csv(\"data/presidents.csv\", index=False, encoding=\"utf-8\")\n",
    "presidents_df = df\n",
    "presidents_df = presidents_df.drop(presidents_df.columns[[0, 1, 3]], axis = 1).iloc[:-2]\n",
    "\n",
    "# Clean term column\n",
    "def clean_term(term):\n",
    "    if pd.isna(term):\n",
    "        return None\n",
    "\n",
    "    term = str(term).strip().replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "    parts = term.split(\"-\")\n",
    "\n",
    "    # Clean out non-digitc characters (like * † a b etc.)\n",
    "    start = re.sub(r\"\\D\", \"\", parts[0])\n",
    "\n",
    "    if start == \"\":\n",
    "        return None\n",
    "    start = int(start)\n",
    "\n",
    "    # Handle end year\n",
    "    if len(parts) == 1 or parts[1].strip() == \"\":\n",
    "        end = start\n",
    "\n",
    "    else:\n",
    "        end = re.sub(r\"\\D\", \"\", parts[1])\n",
    "\n",
    "        if end == \"\":\n",
    "            end = start\n",
    "\n",
    "        elif len(end) == 2:\n",
    "            end = int(str(start)[:2] + end)\n",
    "\n",
    "        else:\n",
    "            end = int(end)\n",
    "\n",
    "    return f\"{start}–{end}\" if start != end else str(start)\n",
    "\n",
    "presidents_df[\"term\"] = presidents_df[\"term\"].apply(clean_term)\n",
    "presidents_df[[\"term_start\", \"term_end\"]] = (presidents_df[\"term\"].str.split(\"–\", expand=True))\n",
    "presidents_df[\"term_start\"] = presidents_df[\"term_start\"].astype(\"Int64\")\n",
    "presidents_df[\"term_end\"] = presidents_df[\"term_end\"].astype(\"Int64\")\n",
    "\n",
    "presidents_df.to_csv(\"data/presidents.csv\", index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51502f1c-e703-4c6e-b672-9b6192e0f327",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0a7744-acd8-4404-b26a-bccdb08ccb57",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701ab8f-435b-42bd-b7e0-fb27756f60e3",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### **Presidential Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a3141-41b8-462f-a4dd-654c368fd18e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf3151-ecc9-4b57-9172-d0fbc52c9608",
   "metadata": {},
   "source": [
    "To create a corpus of texts to analyze, we scraped a variety of presidential documents from\n",
    "[The American Presidency Project](https://www.presidency.ucsb.edu/documents).\n",
    "This allowed us to collect various text samples either written or delivered by the US presidents during their term in office. Note that at this time, texts have NOT been labeled as either public-facing or internal. Documents were taken from the following categories:\n",
    "\n",
    "  * Eulogies\n",
    "  * Farewell Addresses\n",
    "  * Fireside Chats\n",
    "  * Inaugural Addresses\n",
    "  * Interviews\n",
    "  * Messages\n",
    "  * News Conferences\n",
    "  * Weekly Addresses\n",
    "  * State of Union Addresses\n",
    "\n",
    "Once all documents were collected, we had a sample size of\n",
    "$18,144$\n",
    "texts to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816dac6-8886-45ea-bdf0-59b69e903731",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fabc4c-b707-4e3c-86a7-f7b1350c669e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc4afd-f717-4d5a-a943-6e30e23b630b",
   "metadata": {},
   "source": [
    "#### **Eulogies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b69d0-746b-4fd6-9227-e31a865f0fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------------\n",
    "base_url = \"https://www.presidency.ucsb.edu\"\n",
    "list_url = f\"{base_url}/documents/app-categories/spoken-addresses-and-remarks/presidential/eulogies?items_per_page=60&page=\"\n",
    "total_pages = 2\n",
    "min_delay = 1.5\n",
    "max_delay = 3.5\n",
    "text_category = \"eulogy\"\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 1: Scrape Eulogy List\n",
    "# ----------------------------------------------\n",
    "def scrape_eulogy_list():\n",
    "    all_items = []\n",
    "\n",
    "    for page in tqdm(range(total_pages), desc=\"Pages\"):\n",
    "        url = f\"{list_url}{page}\"\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        eulogies = soup.find_all(\"div\", class_=\"node-documents\")\n",
    "\n",
    "        for item in eulogies:\n",
    "            try:\n",
    "                date = item.find(\"span\", {\"property\": \"dc:date\"}).get_text(strip=True)\n",
    "                title_tag = item.find(\"div\", class_=\"field-title\").find(\"a\")\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                href = base_url + title_tag[\"href\"]\n",
    "                president = item.find(\"div\", class_=\"col-sm-4\").find(\"a\").get_text(strip=True)\n",
    "\n",
    "                all_items.append({\n",
    "                    \"date\": date,\n",
    "                    \"title\": title,\n",
    "                    \"url\": href,\n",
    "                    \"president\": president,\n",
    "                    \"text_category\": text_category\n",
    "                })\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "    return all_items\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 2: Scrape Full Text\n",
    "# ----------------------------------------------\n",
    "def scrape_full_text(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"field-docs-content\")\n",
    "        if not content:\n",
    "            return None\n",
    "\n",
    "        paragraphs = [p.get_text(\" \", strip=True) for p in content.find_all(\"p\")]\n",
    "        return \"\\n\".join(paragraphs)\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 3: Main\n",
    "# ----------------------------------------------\n",
    "def main():\n",
    "    # Step 3a: Scrape list of eulogies\n",
    "    eulogies = scrape_eulogy_list()\n",
    "    df = pd.DataFrame(eulogies)\n",
    "\n",
    "    # Step 3b: Scrape full text with progress bar\n",
    "    full_texts = []\n",
    "    for row in tqdm(df.itertuples(index=False), total=len(df), desc=\"Scraping full text\"):\n",
    "        text = scrape_full_text(row.url)\n",
    "        full_texts.append(text)\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "\n",
    "    df[\"full_text\"] = full_texts\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"data/eulogy.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e54f626-5aa2-4519-af67-29913446f73c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1da63-1c62-4933-baa8-32d3e946f063",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd92f51-69cb-4939-9e58-816ee5a80079",
   "metadata": {},
   "source": [
    "#### **Farewell Addresses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b237a7-8668-40ca-a8c2-2da34d3ea39f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------------\n",
    "base_url = \"https://www.presidency.ucsb.edu\"\n",
    "list_url = f\"{base_url}/documents/app-categories/spoken-addresses-and-remarks/presidential/farewell-addresses?items_per_page=10&page=\"\n",
    "total_pages = 2\n",
    "min_delay = 1.5\n",
    "max_delay = 3.5\n",
    "text_category = \"farewell address\"\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 1: Scrape Farewell Address List\n",
    "# ----------------------------------------------\n",
    "def scrape_farewell_address_list():\n",
    "    all_items = []\n",
    "\n",
    "    for page in tqdm(range(total_pages), desc=\"Pages\"):\n",
    "        url = f\"{list_url}{page}\"\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        farewell_addresses = soup.find_all(\"div\", class_=\"node-documents\")\n",
    "\n",
    "        for item in farewell_addresses:\n",
    "            try:\n",
    "                date = item.find(\"span\", {\"property\": \"dc:date\"}).get_text(strip=True)\n",
    "                title_tag = item.find(\"div\", class_=\"field-title\").find(\"a\")\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                href = base_url + title_tag[\"href\"]\n",
    "                president = item.find(\"div\", class_=\"col-sm-4\").find(\"a\").get_text(strip=True)\n",
    "\n",
    "                all_items.append({\n",
    "                    \"date\": date,\n",
    "                    \"title\": title,\n",
    "                    \"url\": href,\n",
    "                    \"president\": president,\n",
    "                    \"text_category\": text_category\n",
    "                })\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "    return all_items\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 2: Scrape Full Text\n",
    "# ----------------------------------------------\n",
    "def scrape_full_text(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"field-docs-content\")\n",
    "        if not content:\n",
    "            return None\n",
    "\n",
    "        paragraphs = [p.get_text(\" \", strip=True) for p in content.find_all(\"p\")]\n",
    "        return \"\\n\".join(paragraphs)\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 3: Main\n",
    "# ----------------------------------------------\n",
    "def main():\n",
    "    # Step 3a: Scrape list of farewell_addresses\n",
    "    farewell_addresses = scrape_farewell_address_list()\n",
    "    df = pd.DataFrame(farewell_addresses)\n",
    "\n",
    "    # Step 3b: Scrape full text with progress bar\n",
    "    full_texts = []\n",
    "    for row in tqdm(df.itertuples(index=False), total=len(df), desc=\"Scraping full text\"):\n",
    "        text = scrape_full_text(row.url)\n",
    "        full_texts.append(text)\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "\n",
    "    df[\"full_text\"] = full_texts\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"data/farewell_address.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307d72f4-7e65-45bd-8385-7ea661e5efb2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3db0e-de1f-4138-8560-8c5d8a3c9534",
   "metadata": {},
   "source": [
    "#### **Fireside Chats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584ffa3-e8fa-45bb-b17c-f2dbf902edfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------------\n",
    "base_url = \"https://www.presidency.ucsb.edu\"\n",
    "list_url = f\"{base_url}/documents/app-categories/spoken-addresses-and-remarks/presidential/fireside-chats?items_per_page=20&page=\"\n",
    "total_pages = 2\n",
    "min_delay = 1.5\n",
    "max_delay = 3.5\n",
    "text_category = \"fireside chat\"\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 1: Scrape Fireside Chat List\n",
    "# ----------------------------------------------\n",
    "def scrape_fireside_chat_list():\n",
    "    all_items = []\n",
    "\n",
    "    for page in tqdm(range(total_pages), desc=\"Pages\"):\n",
    "        url = f\"{list_url}{page}\"\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        fireside_chats = soup.find_all(\"div\", class_=\"node-documents\")\n",
    "\n",
    "        for item in fireside_chats:\n",
    "            try:\n",
    "                date = item.find(\"span\", {\"property\": \"dc:date\"}).get_text(strip=True)\n",
    "                title_tag = item.find(\"div\", class_=\"field-title\").find(\"a\")\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                href = base_url + title_tag[\"href\"]\n",
    "                president = item.find(\"div\", class_=\"col-sm-4\").find(\"a\").get_text(strip=True)\n",
    "\n",
    "                all_items.append({\n",
    "                    \"date\": date,\n",
    "                    \"title\": title,\n",
    "                    \"url\": href,\n",
    "                    \"president\": president,\n",
    "                    \"text_category\": text_category\n",
    "                })\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "    return all_items\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 2: Scrape Full Text\n",
    "# ----------------------------------------------\n",
    "def scrape_full_text(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"field-docs-content\")\n",
    "        if not content:\n",
    "            return None\n",
    "\n",
    "        paragraphs = [p.get_text(\" \", strip=True) for p in content.find_all(\"p\")]\n",
    "        return \"\\n\".join(paragraphs)\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 3: Main\n",
    "# ----------------------------------------------\n",
    "def main():\n",
    "    # Step 3a: Scrape list of fireside_chats\n",
    "    fireside_chats = scrape_fireside_chat_list()\n",
    "    df = pd.DataFrame(fireside_chats)\n",
    "\n",
    "    # Step 3b: Scrape full text with progress bar\n",
    "    full_texts = []\n",
    "    for row in tqdm(df.itertuples(index=False), total=len(df), desc=\"Scraping full text\"):\n",
    "        text = scrape_full_text(row.url)\n",
    "        full_texts.append(text)\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "\n",
    "    df[\"full_text\"] = full_texts\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"data/fireside_chat.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f2fff-cdd9-43f5-b2af-76ed05bc7f0c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68cb47-77ba-4e22-a733-871dfc041f2c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb75125-15b2-45b8-8f6c-828157a0c4fc",
   "metadata": {},
   "source": [
    "#### **Inaugural Addresses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf58f1f-b32f-4ef7-9f13-26b268a69a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------------\n",
    "base_url = \"https://www.presidency.ucsb.edu\"\n",
    "list_url = f\"{base_url}/documents/app-categories/spoken-addresses-and-remarks/presidential/inaugural-addresses?items_per_page=60&page=\"\n",
    "total_pages = 2\n",
    "min_delay = 1.5\n",
    "max_delay = 3.5\n",
    "text_category = \"inaugural address\"\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 1: Scrape Inaugural Address List\n",
    "# ----------------------------------------------\n",
    "def scrape_inaugural_address_list():\n",
    "    all_items = []\n",
    "\n",
    "    for page in tqdm(range(total_pages), desc=\"Pages\"):\n",
    "        url = f\"{list_url}{page}\"\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        inaugural_addresses = soup.find_all(\"div\", class_=\"node-documents\")\n",
    "\n",
    "        for item in inaugural_addresses:\n",
    "            try:\n",
    "                date = item.find(\"span\", {\"property\": \"dc:date\"}).get_text(strip=True)\n",
    "                title_tag = item.find(\"div\", class_=\"field-title\").find(\"a\")\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                href = base_url + title_tag[\"href\"]\n",
    "                president = item.find(\"div\", class_=\"col-sm-4\").find(\"a\").get_text(strip=True)\n",
    "\n",
    "                all_items.append({\n",
    "                    \"date\": date,\n",
    "                    \"title\": title,\n",
    "                    \"url\": href,\n",
    "                    \"president\": president,\n",
    "                    \"text_category\": text_category\n",
    "                })\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "    return all_items\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 2: Scrape Full Text\n",
    "# ----------------------------------------------\n",
    "def scrape_full_text(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"field-docs-content\")\n",
    "        if not content:\n",
    "            return None\n",
    "\n",
    "        paragraphs = [p.get_text(\" \", strip=True) for p in content.find_all(\"p\")]\n",
    "        return \"\\n\".join(paragraphs)\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 3: Main\n",
    "# ----------------------------------------------\n",
    "def main():\n",
    "    # Step 3a: Scrape list of inaugural_addresses\n",
    "    inaugural_addresses = scrape_inaugural_address_list()\n",
    "    df = pd.DataFrame(inaugural_addresses)\n",
    "\n",
    "    # Step 3b: Scrape full text with progress bar\n",
    "    full_texts = []\n",
    "    for row in tqdm(df.itertuples(index=False), total=len(df), desc=\"Scraping full text\"):\n",
    "        text = scrape_full_text(row.url)\n",
    "        full_texts.append(text)\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "\n",
    "    df[\"full_text\"] = full_texts\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"data/inaugural_address.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596bfbed-25ea-44e0-bfe3-5f392030e0cf",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327bd7e8-1fc3-4a9d-a026-6452c2eb25ea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8a233-0007-46dc-b7cd-de314b1f76b1",
   "metadata": {},
   "source": [
    "#### **Interviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6b31f-562f-42a8-a7bd-cb8d91bce628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------------\n",
    "base_url = \"https://www.presidency.ucsb.edu\"\n",
    "list_url = f\"{base_url}/documents/app-categories/presidential/interviews?items_per_page=60&page=\"\n",
    "total_pages = 18\n",
    "min_delay = 1.5\n",
    "max_delay = 3.5\n",
    "text_category = \"interview\"\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 1: Scrape Interview List\n",
    "# ----------------------------------------------\n",
    "def scrape_interview_list():\n",
    "    all_items = []\n",
    "\n",
    "    for page in tqdm(range(total_pages), desc=\"Pages\"):\n",
    "        url = f\"{list_url}{page}\"\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        interviews = soup.find_all(\"div\", class_=\"node-documents\")\n",
    "\n",
    "        for item in interviews:\n",
    "            try:\n",
    "                date = item.find(\"span\", {\"property\": \"dc:date\"}).get_text(strip=True)\n",
    "                title_tag = item.find(\"div\", class_=\"field-title\").find(\"a\")\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                href = base_url + title_tag[\"href\"]\n",
    "                president = item.find(\"div\", class_=\"col-sm-4\").find(\"a\").get_text(strip=True)\n",
    "\n",
    "                all_items.append({\n",
    "                    \"date\": date,\n",
    "                    \"title\": title,\n",
    "                    \"url\": href,\n",
    "                    \"president\": president,\n",
    "                    \"text_category\": text_category\n",
    "                })\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "    return all_items\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 2: Scrape Full Text\n",
    "# ----------------------------------------------\n",
    "def scrape_full_text(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"field-docs-content\")\n",
    "        if not content:\n",
    "            return None\n",
    "\n",
    "        paragraphs = [p.get_text(\" \", strip=True) for p in content.find_all(\"p\")]\n",
    "        return \"\\n\".join(paragraphs)\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 3: Main\n",
    "# ----------------------------------------------\n",
    "def main():\n",
    "    # Step 3a: Scrape list of interviews\n",
    "    interviews = scrape_interview_list()\n",
    "    df = pd.DataFrame(interviews)\n",
    "\n",
    "    # Step 3b: Scrape full text with progress bar\n",
    "    full_texts = []\n",
    "    for row in tqdm(df.itertuples(index=False), total=len(df), desc=\"Scraping full text\"):\n",
    "        text = scrape_full_text(row.url)\n",
    "        full_texts.append(text)\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "\n",
    "    df[\"full_text\"] = full_texts\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"data/interview.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa20a17-b847-4c5c-b0f3-54854725a6aa",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a59f77-d062-46b5-b26d-93d80e6a44d6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a597f-e611-4f12-acbf-e3f7cd7afbd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Messages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb71af-3065-4716-9bd9-73b94c8488c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------------\n",
    "base_url = \"https://www.presidency.ucsb.edu\"\n",
    "list_url = f\"{base_url}/documents/app-categories/citations/presidential/messages?items_per_page=60&page=\"\n",
    "total_pages = 212\n",
    "min_delay = 1.5\n",
    "max_delay = 3.5\n",
    "text_category = \"message\"\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 1: Scrape Message List\n",
    "# ----------------------------------------------\n",
    "def scrape_message_list():\n",
    "    all_items = []\n",
    "\n",
    "    for page in tqdm(range(total_pages), desc=\"Pages\"):\n",
    "        url = f\"{list_url}{page}\"\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        messages = soup.find_all(\"div\", class_=\"node-documents\")\n",
    "\n",
    "        for item in messages:\n",
    "            try:\n",
    "                date = item.find(\"span\", {\"property\": \"dc:date\"}).get_text(strip=True)\n",
    "                title_tag = item.find(\"div\", class_=\"field-title\").find(\"a\")\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                href = base_url + title_tag[\"href\"]\n",
    "                president = item.find(\"div\", class_=\"col-sm-4\").find(\"a\").get_text(strip=True)\n",
    "\n",
    "                all_items.append({\n",
    "                    \"date\": date,\n",
    "                    \"title\": title,\n",
    "                    \"url\": href,\n",
    "                    \"president\": president,\n",
    "                    \"text_category\": text_category\n",
    "                })\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "    return all_items\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 2: Scrape Full Text\n",
    "# ----------------------------------------------\n",
    "def scrape_full_text(url):\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            url,\n",
    "            headers={\"User-Agent\": \"Mozilla/5.0\"},\n",
    "            timeout=15\n",
    "        )\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"field-docs-content\")\n",
    "        if not content:\n",
    "            return None\n",
    "\n",
    "        paragraphs = [p.get_text(\" \", strip=True) for p in content.find_all(\"p\")]\n",
    "        return \"\\n\".join(paragraphs)\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 3: Main\n",
    "# ----------------------------------------------\n",
    "def main():\n",
    "    # Step 3a: Scrape list of messages\n",
    "    messages = scrape_message_list()\n",
    "    df = pd.DataFrame(messages)\n",
    "\n",
    "    # Step 3b: Scrape full text with progress bar\n",
    "    full_texts = []\n",
    "    for row in tqdm(df.itertuples(index=False), total=len(df), desc=\"Scraping full text\"):\n",
    "        text = scrape_full_text(row.url)\n",
    "        full_texts.append(text)\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "\n",
    "    df[\"full_text\"] = full_texts\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"data/message.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0724a-f4fa-4238-8b34-bb88edc48c07",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a74bb-6979-42b4-a109-b656baeb8d49",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec99eed-8c64-48ae-9c2b-db3fb63bdd95",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **News Conferences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf3b97-156c-421e-b445-ce3fb50a896f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------------\n",
    "base_url = \"https://www.presidency.ucsb.edu\"\n",
    "list_url = f\"{base_url}/documents/app-categories/presidential/news-conferences?items_per_page=60&page=\"\n",
    "total_pages = 43\n",
    "min_delay = 1.5\n",
    "max_delay = 3.5\n",
    "text_category = \"news conference\"\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 1: Scrape News Conference List\n",
    "# ----------------------------------------------\n",
    "def scrape_news_conference_list():\n",
    "    all_items = []\n",
    "\n",
    "    for page in tqdm(range(total_pages), desc=\"Pages\"):\n",
    "        url = f\"{list_url}{page}\"\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        news_conferences = soup.find_all(\"div\", class_=\"node-documents\")\n",
    "\n",
    "        for item in news_conferences:\n",
    "            try:\n",
    "                date = item.find(\"span\", {\"property\": \"dc:date\"}).get_text(strip=True)\n",
    "                title_tag = item.find(\"div\", class_=\"field-title\").find(\"a\")\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                href = base_url + title_tag[\"href\"]\n",
    "                president = item.find(\"div\", class_=\"col-sm-4\").find(\"a\").get_text(strip=True)\n",
    "\n",
    "                all_items.append({\n",
    "                    \"date\": date,\n",
    "                    \"title\": title,\n",
    "                    \"url\": href,\n",
    "                    \"president\": president,\n",
    "                    \"text_category\": text_category\n",
    "                })\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "    return all_items\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 2: Scrape Full Text\n",
    "# ----------------------------------------------\n",
    "def scrape_full_text(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"field-docs-content\")\n",
    "        if not content:\n",
    "            return None\n",
    "\n",
    "        paragraphs = [p.get_text(\" \", strip=True) for p in content.find_all(\"p\")]\n",
    "        return \"\\n\".join(paragraphs)\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 3: Main\n",
    "# ----------------------------------------------\n",
    "def main():\n",
    "    # Step 3a: Scrape list of news_conferences\n",
    "    news_conferences = scrape_news_conference_list()\n",
    "    df = pd.DataFrame(news_conferences)\n",
    "\n",
    "    # Step 3b: Scrape full text with progress bar\n",
    "    full_texts = []\n",
    "    for row in tqdm(df.itertuples(index=False), total=len(df), desc=\"Scraping full text\"):\n",
    "        text = scrape_full_text(row.url)\n",
    "        full_texts.append(text)\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "\n",
    "    df[\"full_text\"] = full_texts\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"data/news_conference.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f2513-77b3-44be-95ba-5e874c5a8537",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999f11e1-5f24-4669-9e99-dc914257d286",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2693bbc9-6bac-4038-a506-ac61e53ccd7a",
   "metadata": {},
   "source": [
    "#### **Weekly Addresses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e126a79-b8be-4bef-aaa6-798ebc949559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------------\n",
    "base_url = \"https://www.presidency.ucsb.edu\"\n",
    "list_url = f\"{base_url}/documents/app-categories/spoken-addresses-and-remarks/presidential/saturday-weekly-addresses?items_per_page=60&page=\"\n",
    "total_pages = 28\n",
    "min_delay = 1.5\n",
    "max_delay = 3.5\n",
    "text_category = \"weekly address\"\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 1: Scrape Weekly Address List\n",
    "# ----------------------------------------------\n",
    "def scrape_weekly_address_list():\n",
    "    all_items = []\n",
    "\n",
    "    for page in tqdm(range(total_pages), desc=\"Pages\"):\n",
    "        url = f\"{list_url}{page}\"\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        weekly_addresses = soup.find_all(\"div\", class_=\"node-documents\")\n",
    "\n",
    "        for item in weekly_addresses:\n",
    "            try:\n",
    "                date = item.find(\"span\", {\"property\": \"dc:date\"}).get_text(strip=True)\n",
    "\n",
    "                title_tag = item.find(\"div\", class_=\"field-title\").find(\"a\")\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                href = base_url + title_tag[\"href\"]\n",
    "                president = item.find(\"div\", class_=\"col-sm-4\").find(\"a\").get_text(strip=True)\n",
    "\n",
    "                all_items.append({\n",
    "                    \"date\": date,\n",
    "                    \"title\": title,\n",
    "                    \"url\": href,\n",
    "                    \"president\": president,\n",
    "                    \"text_category\": text_category\n",
    "                })\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "    return all_items\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 2: Scrape Full Text\n",
    "# ----------------------------------------------\n",
    "def scrape_full_text(url):\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            url,\n",
    "            headers={\"User-Agent\": \"Mozilla/5.0\"},\n",
    "            timeout=15\n",
    "        )\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"field-docs-content\")\n",
    "        if not content:\n",
    "            return None\n",
    "\n",
    "        paragraphs = [p.get_text(\" \", strip=True) for p in content.find_all(\"p\")]\n",
    "        return \"\\n\".join(paragraphs)\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 3: Main\n",
    "# ----------------------------------------------\n",
    "def main():\n",
    "    # Step 3a: Scrape list of weekly_addresses\n",
    "    weekly_addresses = scrape_weekly_address_list()\n",
    "    df = pd.DataFrame(weekly_addresses)\n",
    "\n",
    "    # Step 3b: Scrape full text with progress bar\n",
    "    full_texts = []\n",
    "    for row in tqdm(df.itertuples(index=False), total=len(df), desc=\"Scraping full text\"):\n",
    "        text = scrape_full_text(row.url)\n",
    "        full_texts.append(text)\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "\n",
    "    df[\"full_text\"] = full_texts\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"data/weekly_address.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0784ed8-b39a-479d-b41e-b31eb42a4136",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84697e35-4b14-4a28-920e-fb22a29736bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282d53a-252a-4f52-912a-57a1ffb638bf",
   "metadata": {},
   "source": [
    "#### **State of the Union Addresses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4159e32-7e07-46c4-b443-f2c7b65610d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------------\n",
    "base_url = \"https://www.presidency.ucsb.edu\"\n",
    "list_url = f\"{base_url}/documents/app-categories/spoken-addresses-and-remarks/presidential/state-the-union-addresses?items_per_page=60&page=\"\n",
    "total_pages = 2\n",
    "min_delay = 1.5\n",
    "max_delay = 3.5\n",
    "text_category = \"State of the Union Address\"\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 1: Scrape State of the Union Address List\n",
    "# ----------------------------------------------\n",
    "def scrape_sotu_address_list():\n",
    "    all_items = []\n",
    "\n",
    "    for page in tqdm(range(total_pages), desc=\"Pages\"):\n",
    "        url = f\"{list_url}{page}\"\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        sotu_addresses = soup.find_all(\"div\", class_=\"node-documents\")\n",
    "\n",
    "        for item in sotu_addresses:\n",
    "            try:\n",
    "                date = item.find(\"span\", {\"property\": \"dc:date\"}).get_text(strip=True)\n",
    "\n",
    "                title_tag = item.find(\"div\", class_=\"field-title\").find(\"a\")\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                href = base_url + title_tag[\"href\"]\n",
    "                president = item.find(\"div\", class_=\"col-sm-4\").find(\"a\").get_text(strip=True)\n",
    "\n",
    "                all_items.append({\n",
    "                    \"date\": date,\n",
    "                    \"title\": title,\n",
    "                    \"url\": href,\n",
    "                    \"president\": president,\n",
    "                    \"text_category\": text_category\n",
    "                })\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "    return all_items\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 2: Scrape Full Text\n",
    "# ----------------------------------------------\n",
    "def scrape_full_text(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"field-docs-content\")\n",
    "        if not content:\n",
    "            return None\n",
    "\n",
    "        paragraphs = [p.get_text(\" \", strip=True) for p in content.find_all(\"p\")]\n",
    "        return \"\\n\".join(paragraphs)\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 3: Main\n",
    "# ----------------------------------------------\n",
    "def main():\n",
    "    # Step 3a: Scrape list of sotu_addresses\n",
    "    sotu_addresses = scrape_sotu_address_list()\n",
    "    df = pd.DataFrame(sotu_addresses)\n",
    "\n",
    "    # Step 3b: Scrape full text with progress bar\n",
    "    full_texts = []\n",
    "    for row in tqdm(df.itertuples(index=False), total=len(df), desc=\"Scraping full text\"):\n",
    "        text = scrape_full_text(row.url)\n",
    "        full_texts.append(text)\n",
    "        time.sleep(uniform(min_delay, max_delay))\n",
    "\n",
    "    df[\"full_text\"] = full_texts\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"data/sotu_address.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21266f77-1d4c-4e4d-8782-742c3c03d165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
